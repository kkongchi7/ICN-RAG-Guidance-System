{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ajSjeVbGWA2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb sentence-transformers transformers openai \"urllib3<2.0\" tqdm ujson orjson pandas -q"
      ],
      "metadata": {
        "id": "LcPLHYOm8YzR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import os\n",
        "import chromadb\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import Dict, Any, List\n",
        "import csv\n",
        "import time\n",
        "from datetime import date, datetime, timedelta, timezone\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import ast\n",
        "import sys\n",
        "import importlib\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output"
      ],
      "metadata": {
        "id": "JgoJJwSci_pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMmB-5V5YOM2"
      },
      "source": [
        "# **ì¸ì²œê³µí•­ ê³µí•­ì‹œì„¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1TMmmftYFro"
      },
      "outputs": [],
      "source": [
        "# ìš”ì²­ì„ ë³´ë‚¼ URL\n",
        "url = \"https://icnmap.airport.kr:18080/API/v2_0/nodes/no-auth/web/1\"\n",
        "\n",
        "try:\n",
        "    # GET ìš”ì²­ ë³´ë‚´ê¸°\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # HTTP ì‘ë‹µ ì½”ë“œ í™•ì¸ (200ì€ ì„±ê³µ)\n",
        "    if response.status_code == 200:\n",
        "        print(\"ìš”ì²­ ì„±ê³µ!\")\n",
        "\n",
        "        # ì‘ë‹µ ë³¸ë¬¸ì„ JSON í˜•ì‹ìœ¼ë¡œ íŒŒì‹±\n",
        "        data = response.json()\n",
        "\n",
        "        # íŒŒì‹±ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n",
        "        file_path = \"airport_data.json\"\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ {file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"ìš”ì²­ ì‹¤íŒ¨. ìƒíƒœ ì½”ë“œ: {response.status_code}\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "# 1. ê¸°ì¡´ JSON íŒŒì¼ ì½ê¸°\n",
        "with open(\"airport_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    airport_data = json.load(f)\n",
        "\n",
        "# 2. spoiInfors ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼\n",
        "spoi_infors = airport_data[\"data\"][\"spoiInfors\"]\n",
        "\n",
        "# 3. ê° í•­ëª©ì˜ nameì—ì„œ í•œê¸€ë§Œ ë‚¨ê¸°ê¸°\n",
        "for spo in spoi_infors:\n",
        "    try:\n",
        "        name_list = json.loads(spo[\"name\"])  # ë¬¸ìì—´ -> JSON ë°°ì—´\n",
        "        # locale == \"ko\"ë§Œ í•„í„°ë§\n",
        "        ko_list = [item for item in name_list if item.get(\"locale\") == \"ko\"]\n",
        "        # ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ì €ì¥\n",
        "        spo[\"name\"] = json.dumps(ko_list, ensure_ascii=False)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"spo vid={spo.get('vid')} name í•„ë“œ íŒŒì‹± ì‹¤íŒ¨\")\n",
        "\n",
        "# 4. ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
        "with open(\"airport_data_ko.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(airport_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"ìƒˆ íŒŒì¼ 'airport_data_ko.json' ìƒì„± ì™„ë£Œ\")\n",
        "\n",
        "# 5. ê¸°ì¡´ íŒŒì¼ ì½ê¸°\n",
        "with open(\"airport_data_ko.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    airport_data = json.load(f)\n",
        "\n",
        "# 6. data ì•ˆì—ì„œ spoiInforsë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ í‚¤ ì œê±°\n",
        "if \"data\" in airport_data and \"spoiInfors\" in airport_data[\"data\"]:\n",
        "    airport_data[\"data\"] = {\"spoiInfors\": airport_data[\"data\"][\"spoiInfors\"]}\n",
        "\n",
        "# 7. ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
        "with open(\"airport_data_ko_poi.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(airport_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"ìƒˆ íŒŒì¼ 'airport_data_ko_poi.json' ìƒì„± ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCrF3A3AYylX"
      },
      "source": [
        "**í¬ë¡¤ë§ ë°ì´í„° êµ¬ì¡° ì¬ë°°ì—´í•˜ê¸°**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_facility_data(\n",
        "    input_candidates=[\"airport_data.json\", \"/content/airport_data_ko_poi.json\"],\n",
        "    output_path=\"/content/spoi_formatted_with_category.json\",\n",
        "    report_path=\"/content/spoi_category_patch_report.json\",\n",
        "    fetch_remote=False\n",
        "):\n",
        "    if fetch_remote:\n",
        "        url = \"https://icnmap.airport.kr:18080/API/v2_0/nodes/no-auth/web/1\"\n",
        "        try:\n",
        "            response = requests.get(url, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                print(\"ìš”ì²­ ì„±ê³µ!\")\n",
        "                data = response.json()\n",
        "                file_path = \"airport_data.json\"\n",
        "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "                print(f\"ë°ì´í„°ê°€ {file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "            else:\n",
        "                print(f\"ìš”ì²­ ì‹¤íŒ¨: {response.status_code}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "    # =========================\n",
        "    # ê²½ë¡œ ì„¤ì •\n",
        "    # =========================\n",
        "    INPUT_PATH = next((p for p in input_candidates if Path(p).exists()), input_candidates[-1])\n",
        "    assert Path(INPUT_PATH).exists(), f\"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {INPUT_PATH}\"\n",
        "\n",
        "    # =========================\n",
        "    # ë‚´ë¶€ ìœ í‹¸ í•¨ìˆ˜ë“¤\n",
        "    # =========================\n",
        "    def load_json(path): return json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "    def get_spoi_list(root):\n",
        "        if isinstance(root, dict):\n",
        "            if isinstance(root.get(\"spoiInfors\"), list): return root[\"spoiInfors\"]\n",
        "            data = root.get(\"data\")\n",
        "            if isinstance(data, dict) and isinstance(data.get(\"spoiInfors\"), list):\n",
        "                return data[\"spoiInfors\"]\n",
        "        return []\n",
        "\n",
        "    def parse_name_field(name_field):\n",
        "        if name_field is None: return []\n",
        "        if isinstance(name_field, list): return name_field\n",
        "        if isinstance(name_field, str):\n",
        "            s = name_field.strip()\n",
        "            if not s: return []\n",
        "            try:\n",
        "                parsed = json.loads(s)\n",
        "                if isinstance(parsed, list): return parsed\n",
        "                if isinstance(parsed, dict): return [parsed]\n",
        "            except Exception: return []\n",
        "        return []\n",
        "\n",
        "    def pick_best_locale(inner_list):\n",
        "        if not inner_list: return None\n",
        "        for it in inner_list:\n",
        "            if (it or {}).get(\"locale\") == \"ko\":\n",
        "                return it\n",
        "        return inner_list[0]\n",
        "\n",
        "    def map_building(building_id):\n",
        "        mapping = {1: \"ì œ1ì—¬ê°í„°ë¯¸ë„\", 2: \"ì œ2ì—¬ê°í„°ë¯¸ë„\", 3: \"íƒ‘ìŠ¹ë™\"}\n",
        "        return mapping.get(building_id, building_id)\n",
        "\n",
        "    def extract_floor(loc_desc):\n",
        "        if not loc_desc: return None\n",
        "        s = str(loc_desc)\n",
        "        for pat in [r\"(ì§€í•˜\\s*\\d+\\s*ì¸µ)\", r\"(\\d+\\s*ì¸µ)\", r\"([BbFf]\\s*\\d+\\s*ì¸µ)\"]:\n",
        "            m = re.search(pat, s)\n",
        "            if m: return re.sub(r\"\\s+\", \"\", m.group(1)).upper()\n",
        "        return None\n",
        "\n",
        "    def hhmm_to_colon(t):\n",
        "        if t is None: return None\n",
        "        s = str(t).strip()\n",
        "        if not s.isdigit(): return s\n",
        "        s = s.zfill(4)\n",
        "        return f\"{s[:2]}:{s[2:]}\"\n",
        "\n",
        "    def normalize_time_field(t):\n",
        "        if t is None: return None\n",
        "        s = str(t).strip()\n",
        "        if s == \"0000\": return \"00:00\"\n",
        "        return hhmm_to_colon(s)\n",
        "\n",
        "    def normalize_tel(t): return t or \"-\"\n",
        "\n",
        "    # =========================\n",
        "    # category ê·œì¹™\n",
        "    # =========================\n",
        "    def is_blank_category(cat):\n",
        "        return cat in (None, \"-\", \"\", \"null\", \"None\")\n",
        "\n",
        "    def is_numeric_name(name):\n",
        "        if name is None: return False\n",
        "        return bool(re.fullmatch(r\"\\d+\", str(name).strip()))\n",
        "\n",
        "    FOOD_SEEDS = [\"êµ­ìˆ˜\", \"ê³ ê¸°\", \"ìš”ë¦¬\", \"ë°¥\", \"ë²„ê±°\", \"ëˆê¹ŒìŠ¤\"]\n",
        "\n",
        "    def apply_primary_rules(rec):\n",
        "        name, goods = rec.get(\"name\"), rec.get(\"goods\")\n",
        "        if is_numeric_name(name):\n",
        "            rec[\"name\"] = f\"{name}ë²ˆ ì¶œì…êµ¬\"\n",
        "            rec[\"category\"] = \"ì¶œì…êµ¬\"\n",
        "            return True, \"pattern:ì¶œì…êµ¬\"   # â† ë³´ê³ ì„œì—ì„œë„ \"ì¶œì…êµ¬\"ë¡œ ì°íˆê²Œ ìˆ˜ì •\n",
        "        return False, None\n",
        "\n",
        "\n",
        "    PATTERNS = [\n",
        "        (r\"í™”ì¥ì‹¤|restroom|toilet\",           \"í™”ì¥ì‹¤\"),\n",
        "        (r\"ëŒ€í”¼ì†Œ\",                           \"ëŒ€í”¼ì†Œ\"),\n",
        "        (r\"í™˜ìŠ¹ì¥\",                           \"í™˜ìŠ¹ì¥\"),\n",
        "        (r\"ë³´í—˜\",                             \"ë³´í—˜\"),\n",
        "        (r\"ë„ì„œ|ì‹ ë¬¸|ì¡ì§€\",              \"ì„œì \"),\n",
        "        (r\"í¡ì—°|ì „ìë‹´ë°°\",                    \"í¡ì—°ì‹¤\"),\n",
        "        (r\"í˜¸í…”|ê°ì‹¤\",                        \"ìˆ™ë°•ì‹œì„¤/í˜¸í…”\"),\n",
        "        (r\"ì•ˆê²½|ë Œì¦ˆ\",                        \"ì•ˆê²½ì \"),\n",
        "        (r\"ë¼ìš´ì§€|lounge|ìŠ¤ì¹´ì´í—ˆë¸Œ|KAL|ì•„ì‹œì•„ë‚˜.*ë¼ìš´ì§€\", \"ë¼ìš´ì§€\"),\n",
        "        (r\"ë©´ì„¸|duty ?free|ì‹ ë¼ë©´ì„¸ì |ë¡¯ë°ë©´ì„¸ì |ì‹ ì„¸ê³„ë©´ì„¸ì \", \"ë©´ì„¸ì \"),\n",
        "        (r\"ë³´ì•ˆê²€ìƒ‰|ê²€ìƒ‰ëŒ€|security\\s*check\", \"ë³´ì•ˆê²€ìƒ‰\"),\n",
        "        (r\"ì²´í¬ì¸ì¹´ìš´í„°\",                     \"ì²´í¬ì¸ì¹´ìš´í„°\"),\n",
        "        (r\"ì…êµ­ì‹¬ì‚¬|immigration.*arriv\",      \"ì…êµ­ì‹¬ì‚¬\"),\n",
        "        (r\"ì¶œêµ­ì‹¬ì‚¬|immigration.*depart\",     \"ì¶œêµ­ì‹¬ì‚¬\"),\n",
        "        (r\"ì…êµ­ì¥|ë„ì°©ì¥|arrivals?\",           \"ì…êµ­ì¥\"),\n",
        "        (r\"ì¶œêµ­ì¥|ì¶œë°œì¥|departures?\",         \"ì¶œêµ­ì¥\"),\n",
        "        (r\"(ê²Œì´íŠ¸|Gate|íƒ‘ìŠ¹êµ¬)\\s*\\d+\",        \"ê²Œì´íŠ¸/íƒ‘ìŠ¹êµ¬\"),\n",
        "        (r\"ì¶œì…êµ¬\",                            \"ì¶œì…êµ¬\"),\n",
        "        (r\"ì•½êµ­|pharmacy\",                    \"ì•½êµ­/ì˜ë£Œ\"),\n",
        "        (r\"ì˜ë¬´ì‹¤|ì‘ê¸‰|ì˜ë£Œ|clinic|medical\",   \"ì˜ë£Œ\"),\n",
        "        (r\"(ì™€ì´íŒŒì´|wifi|ë¡œë°|ìœ ì‹¬|SIM|í†µì‹ )\", \"í†µì‹ /ë¡œë°\"),\n",
        "        (r\"(ì€í–‰|í™˜ì „|ì™¸í™˜|ATM|ìš°ë¦¬ì€í–‰|ì‹ í•œ|êµ­ë¯¼|ë†í˜‘|í•˜ë‚˜|IBK|KB|NH)\", \"ê¸ˆìœµ/í™˜ì „\"),\n",
        "        (r\"(ìˆ˜í•˜ë¬¼|ìœ ì‹¤ë¬¼|ë³´ê´€ì†Œ?|ë³´ê´€|baggage|lost\\s*and\\s*found|locker)\", \"ìˆ˜í•˜ë¬¼/ë³´ê´€\"),\n",
        "        (r\"(ìœ ì•„|ìˆ˜ìœ ì‹¤|ìˆ˜ìœ |í‚¤ì¦ˆ|ì–´ë¦°ì´|ë†€ì´ì‹œì„¤|stroller)\", \"ìœ ì•„/ì–´ë¦°ì´\"),\n",
        "        (r\"(ì¹´í˜|ì»¤í”¼|cafe|coffee|tea|ë² ì´ì»¤ë¦¬|bakery|ë˜í‚¨|íŒŒë¦¬ë°”ê²Œëœ¨|ëšœë ˆì¥¬ë¥´|í´\\s*ë°”ì…‹|paul\\s*bassett|ìŠ¤íƒ€ë²…ìŠ¤|íˆ¬ì¸|ì´ë””ì•¼|ìŒë£Œ)\", \"ì¹´í˜/ìŒë£Œ\"),\n",
        "        (r\"(KFC|ë§¥ë„ë‚ ë“œ|ë²„ê±°í‚¹|ë¡¯ë°ë¦¬ì•„|ì„œë¸Œì›¨ì´|í”¼ì|íŒŒìŠ¤íƒ€|ìŠ¤ì‹œ|ë¼ë©˜|ë¼ë©´|ìš°ë™|ëˆì¹´ì¸ |í•œì‹|ì¤‘ì‹|ì¼ì‹|ì–‘ì‹|ë¶„ì‹|ì‹ë‹¹|ë ˆìŠ¤í† ë‘|í‘¸ë“œì½”íŠ¸|êµ­ìˆ˜|ê³ ê¸°|ìš”ë¦¬|ë°¥|ë²„ê±°|ëˆê¹ŒìŠ¤)\", \"ì‹ìŒ/ìŒì‹ì /ì‹ë‹¹\"),\n",
        "        (r\"(í¸ì˜ì |CU|GS25|ì„¸ë¸ì¼ë ˆë¸|7-?ELEVEN|ì´ë§ˆíŠ¸24)\", \"í¸ì˜ì \"),\n",
        "        (r\"(ìš°ì²´êµ­|post|ems|íƒë°°|courier|parcel)\", \"ìš°í¸/íƒë°°\"),\n",
        "        (r\"(ìƒ¤ì›Œ|ìƒ¤ì›Œì‹¤|ì‚¬ìš°ë‚˜|ìˆ˜ë©´ì‹¤|ë§ˆì‚¬ì§€|ë¹„ì¦ˆë‹ˆìŠ¤ì„¼í„°|íœ´ê²Œì‹¤)\", \"í¸ì˜ì‹œì„¤\"),\n",
        "        (r\"(ì…”í‹€\\s*íŠ¸ë ˆì¸|ì…”í‹€|ì…”í‹€íŠ¸ë ˆì¸|shuttle\\s*train|ë¬´ì¸ê¶¤ë„ì—´ì°¨|ì—´ì°¨|APM)\", \"ì…”í‹€íŠ¸ë ˆì¸/êµí†µ\"),\n",
        "        (r\"(ë²„ìŠ¤|ë¦¬ë¬´ì§„|ê³µí•­ì² ë„|AREX|ì² ë„|ì§€í•˜ì² |íƒì‹œ|KTX|Rail|Subway)\", \"ëŒ€ì¤‘êµí†µ\"),\n",
        "        (r\"(ì£¼ì°¨|parking)\",                  \"ì£¼ì°¨\"),\n",
        "        (r\"(ë Œí„°ì¹´|ë ŒíŠ¸ì¹´|rental\\s*car|rent-?a-?car)\", \"ë Œí„°ì¹´\"),\n",
        "        (r\"(ê¸°ë…í’ˆ|souvenir|ì„ ë¬¼|ì•¡ì„¸ì„œë¦¬|ì¡í™”|í† ì´|ì™„êµ¬|í™”ì¥í’ˆ|ì½”ìŠ¤ë©”í‹±|íŒ¨ì…˜|ì˜ë¥˜|ìŠˆì¦ˆ|ì‹œê³„|ì¥¬ì–¼ë¦¬|ëª…í’ˆ)\", \"ì‡¼í•‘\"),\n",
        "        (r\"(ì•ˆë‚´|information|help\\s*desk|ì¸í¬|ì—¬ê¶Œ|ë¯¼ì›|ë°œê¸‰|ì‹ ê³ |)\", \"ì•ˆë‚´/í¸ì˜\"),\n",
        "    ]\n",
        "\n",
        "    def apply_extended_rules(rec):\n",
        "        name = str(rec.get(\"name\") or \"\")\n",
        "        goods = str(rec.get(\"goods\") or \"\")\n",
        "        text = f\"{name} {goods}\".strip()\n",
        "        for pat, label in PATTERNS:\n",
        "            if re.search(pat, text, flags=re.IGNORECASE):\n",
        "                rec[\"category\"] = label\n",
        "                return True, f\"pattern:{label}\"\n",
        "        return False, None\n",
        "\n",
        "    # =========================\n",
        "    # floor ë³´ì • ê·œì¹™\n",
        "    # =========================\n",
        "    def is_blank_floor(f):\n",
        "        if f is None:\n",
        "            return True\n",
        "        s = str(f).strip()\n",
        "        return s == \"\" or s == \"-\" or s.lower() == \"null\" or s.lower() == \"none\"\n",
        "\n",
        "    def apply_floor_fill_rules(rec):\n",
        "\n",
        "        # ì´ë¯¸ ì¸µ ì •ë³´ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ \n",
        "        if rec.get(\"floor\") not in (None, \"\", \"-\", \"null\", \"None\"):\n",
        "            return False, None\n",
        "\n",
        "        cat = rec.get(\"category\", \"\")\n",
        "        name = rec.get(\"name\", \"\")\n",
        "        text = (name or \"\") + (cat or \"\")\n",
        "\n",
        "        # âœ… 1. ì…êµ­ì¥ / ë„ì°©ì¥ / ë¡œë¹„ ë“± (1ì¸µ)\n",
        "        if any(k in text for k in [\"ì…êµ­ì¥\", \"ë„ì°©ì¥\", \"ì…êµ­ë¡œë¹„\", \"ë„ì°©ë¡œë¹„\"]):\n",
        "            rec[\"floor\"] = \"1ì¸µ\"\n",
        "            return True, \"floor:arrival_hall\"\n",
        "\n",
        "        # âœ… 2. ë„ì°©ê²Œì´íŠ¸ / ì…êµ­ì‹¬ì‚¬ ë“± (2ì¸µ)\n",
        "        if any(k in text for k in [\"ë„ì°©ê²Œì´íŠ¸\", \"ì…êµ­ì‹¬ì‚¬\", \"ë„ì°©\", \"arrival\"]):\n",
        "            rec[\"floor\"] = \"2ì¸µ\"\n",
        "            return True, \"floor:arrival_gate\"\n",
        "\n",
        "        # âœ… 3. ì²´í¬ì¸ / ì¶œêµ­ / íƒ‘ìŠ¹ ê´€ë ¨ (3ì¸µ)\n",
        "        if any(k in text for k in [\"ì²´í¬ì¸\", \"ì¶œêµ­\", \"ë³´ì•ˆ\", \"íƒ‘ìŠ¹\"]):\n",
        "            rec[\"floor\"] = \"3ì¸µ\"\n",
        "            return True, \"floor:departure\"\n",
        "\n",
        "        # âœ… 4. ìˆ˜í•˜ë¬¼ / ë³´ê´€ ê´€ë ¨ (1ì¸µ)\n",
        "        if any(k in text for k in [\"ìˆ˜í•˜ë¬¼\", \"ìœ ì‹¤ë¬¼\", \"ë³´ê´€ì†Œ\", \"baggage\", \"locker\"]):\n",
        "            rec[\"floor\"] = \"1ì¸µ\"\n",
        "            return True, \"floor:baggage\"\n",
        "\n",
        "        return False, None\n",
        "\n",
        "\n",
        "    # =========================\n",
        "    # ë°ì´í„° ë¶„ë¥˜ ë„¤ì´ë° ì²˜ë¦¬\n",
        "    # =========================\n",
        "    root = load_json(INPUT_PATH)\n",
        "    spoi_list = get_spoi_list(root)\n",
        "    items, patched, rule_counts = [], 0, {}\n",
        "\n",
        "    for spoi in spoi_list:\n",
        "        inner = pick_best_locale(parse_name_field(spoi.get(\"name\"))) or {}\n",
        "        rec = {\n",
        "            \"vsid\": spoi.get(\"dbId\"),\n",
        "            \"name\": inner.get(\"name\"),\n",
        "            \"loc\": inner.get(\"locDesc\"),\n",
        "            \"building\": map_building(spoi.get(\"buildingId\")),\n",
        "            \"floor\": extract_floor(inner.get(\"locDesc\")),\n",
        "            \"category\": \"-\",\n",
        "            \"goods\": inner.get(\"hdlItems\") or inner.get(\"items\"),\n",
        "            \"opnTm\": normalize_time_field(spoi.get(\"opnTm\")),\n",
        "            \"clsTm\": normalize_time_field(spoi.get(\"clsTm\")),\n",
        "            \"telNo\": normalize_tel(spoi.get(\"telNo\")),\n",
        "            \"poiLatitude\": spoi.get(\"poiLatitude\"),\n",
        "            \"poiLongitude\": spoi.get(\"poiLongitude\"),\n",
        "            \"placeImageUrl\": spoi.get(\"placeImageUrl\"),\n",
        "        }\n",
        "\n",
        "        # âœ… ì¹´í…Œê³ ë¦¬ ê·œì¹™\n",
        "        changed_any = False\n",
        "        for fn in [apply_primary_rules, apply_extended_rules]:\n",
        "            changed, rule = fn(rec)\n",
        "            if changed:\n",
        "                changed_any = True\n",
        "                patched += 1\n",
        "                rule_counts[rule] = rule_counts.get(rule, 0) + 1\n",
        "                break\n",
        "\n",
        "        # âœ… ì¸µìˆ˜ ë³´ì •ì€ í•­ìƒ ì‹œë„\n",
        "        changed_floor, rule_floor = apply_floor_fill_rules(rec)\n",
        "        if changed_floor:\n",
        "            patched += 1\n",
        "            rule_counts[rule_floor] = rule_counts.get(rule_floor, 0) + 1\n",
        "\n",
        "        items.append(rec)\n",
        "\n",
        "    # ì €ì¥\n",
        "    out_obj = {\"count\": len(items), \"items\": items}\n",
        "    Path(output_path).write_text(json.dumps(out_obj, ensure_ascii=False, indent=2))\n",
        "    report = {\n",
        "        \"total_items\": len(items),\n",
        "        \"patched_total\": patched,\n",
        "        \"rule_breakdown\": rule_counts,\n",
        "        \"output_path\": output_path,\n",
        "        \"input_path\": INPUT_PATH,\n",
        "    }\n",
        "    Path(report_path).write_text(json.dumps(report, ensure_ascii=False, indent=2))\n",
        "\n",
        "    print(f\"âœ… ì™„ë£Œ: {len(items)}ê°œ ì²˜ë¦¬ / {patched}ê°œ íŒ¨í„´ ë§¤ì¹­ë¨\")\n",
        "\n",
        "format_facility_data()\n"
      ],
      "metadata": {
        "id": "fkXMOP_mQQgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ê³µí•­ì‹œì„¤ ë°ì´í„° ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "dDXg-FPdBcwi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKgZ2OtMQRlA"
      },
      "outputs": [],
      "source": [
        "# ===== ê²½ë¡œ ì„¤ì • =====\n",
        "JSON_PATH = \"/content/spoi_formatted_with_category.json\"  # í˜„ì¬ ê²½ë¡œ ê¸°ì¤€\n",
        "CHROMA_PATH = \"/content/chroma_facilities\"\n",
        "COLLECTION_NAME = \"facilities\"\n",
        "MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
        "\n",
        "# ===== ë©”íƒ€ ì •ë¦¬ ìœ í‹¸ =====\n",
        "def _safe_float(v):\n",
        "    try:\n",
        "        return float(v)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _norm_building(v: str):\n",
        "    if not v:\n",
        "        return None\n",
        "    s = str(v)\n",
        "    if s in (\"ì œ1ì—¬ê°í„°ë¯¸ë„\", \"T1\", \"í„°ë¯¸ë„1\", \"1í„°ë¯¸ë„\"):\n",
        "        return \"ì œ1ì—¬ê°í„°ë¯¸ë„\"\n",
        "    if s in (\"ì œ2ì—¬ê°í„°ë¯¸ë„\", \"T2\", \"í„°ë¯¸ë„2\", \"2í„°ë¯¸ë„\"):\n",
        "        return \"ì œ2ì—¬ê°í„°ë¯¸ë„\"\n",
        "    if \"íƒ‘ìŠ¹ë™\" in s or s in (\"Concourse\", \"íƒ‘ìŠ¹ë™\"):\n",
        "        return \"íƒ‘ìŠ¹ë™\"\n",
        "    return s\n",
        "\n",
        "def _alias_building(v: str):\n",
        "    mp = {\"ì œ1ì—¬ê°í„°ë¯¸ë„\": \"T1\", \"ì œ2ì—¬ê°í„°ë¯¸ë„\": \"T2\", \"íƒ‘ìŠ¹ë™\": \"Concourse\"}\n",
        "    return mp.get(v or \"\", None)\n",
        "\n",
        "def _norm_floor(v):\n",
        "    if v is None:\n",
        "        return None\n",
        "    return str(v).strip()\n",
        "\n",
        "def _std_facility_meta(item: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"ì›ë³¸ JSONì˜ í•„ë“œ êµ¬ì¡°ë¥¼ í‘œì¤€ ë©”íƒ€ êµ¬ì¡°ë¡œ í†µí•©\"\"\"\n",
        "    building_std = _norm_building(item.get(\"building\"))\n",
        "    out = {\n",
        "        \"id\": str(item.get(\"id\") or item.get(\"vsid\") or f\"fac_{item.get('name')}\"),\n",
        "        \"name\": item.get(\"name\") or item.get(\"poiNm\"),\n",
        "        \"building\": building_std,\n",
        "        \"building_alias\": _alias_building(building_std),\n",
        "        \"floor\": _norm_floor(item.get(\"floor\")),\n",
        "        \"category\": item.get(\"category\"),\n",
        "        \"category_norm\": item.get(\"category\") or item.get(\"service\"),\n",
        "        \"loc\": item.get(\"loc\"),\n",
        "        \"lat\": _safe_float(\n",
        "            item.get(\"lat\")\n",
        "            or item.get(\"latitude\")\n",
        "            or item.get(\"poiLatitude\")\n",
        "            or item.get(\"ìœ„ë„\")\n",
        "        ),\n",
        "        \"lon\": _safe_float(\n",
        "            item.get(\"lon\")\n",
        "            or item.get(\"longitude\")\n",
        "            or item.get(\"poiLongitude\")\n",
        "            or item.get(\"ê²½ë„\")\n",
        "        ),\n",
        "        # ë‚˜ë¨¸ì§€ ì¼ë°˜ í•„ë“œ\n",
        "        \"opnTm\": item.get(\"opnTm\"),\n",
        "        \"clsTm\": item.get(\"clsTm\"),\n",
        "        \"telNo\": item.get(\"telNo\"),\n",
        "        \"goods\": item.get(\"goods\"),\n",
        "        \"desc\": item.get(\"desc\"),\n",
        "        \"placeImageUrl\": item.get(\"placeImageUrl\"),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "# ===== í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ + ì„ë² ë”© =====\n",
        "class FacilityEmbeddingFunction:\n",
        "    def __init__(self, model_name=MODEL_NAME):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def preprocess(self, text: str) -> str:\n",
        "        \"\"\"ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì „ì²˜ë¦¬\"\"\"\n",
        "        text = text.lower().strip()\n",
        "        text = text.replace(\"ì œ 1\", \"ì œ1\").replace(\"ì œ 2\", \"ì œ2\")\n",
        "        text = text.replace(\"ì—¬ê° í„°ë¯¸ë„\", \"ì—¬ê°í„°ë¯¸ë„\")\n",
        "        text = text.replace(\"íƒ‘ìŠ¹ ë™\", \"íƒ‘ìŠ¹ë™\")\n",
        "        return text\n",
        "\n",
        "    def __call__(self, texts: List[str]):\n",
        "        pre = [self.preprocess(t) for t in texts]\n",
        "        return self.model.encode(pre, normalize_embeddings=True)\n",
        "\n",
        "# ===== ë³¸ë¬¸ ìƒì„± í•¨ìˆ˜ =====\n",
        "def make_text(item: Dict[str, Any]) -> str:\n",
        "    \"\"\"ì‹œì„¤ ì •ë³´ë¥¼ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ í†µí•©\"\"\"\n",
        "    name = item.get(\"name\", \"\")\n",
        "    cat = item.get(\"category\", \"-\")\n",
        "    bld = item.get(\"building\", \"-\")\n",
        "    flr = item.get(\"floor\", \"-\")\n",
        "    loc = item.get(\"loc\", \"-\")\n",
        "    desc = item.get(\"desc\", \"\")\n",
        "    text = f\"{bld} {flr}ì— ìœ„ì¹˜í•œ {name}ì€(ëŠ”) {cat} ì‹œì„¤ë¡œ, {desc} (ìœ„ì¹˜: {loc})\"\n",
        "    return text\n",
        "\n",
        "# ===== ë©”ì¸ ì„ë² ë”© ë¶€ë¶„ =====\n",
        "def embed_facilities_to_chroma():\n",
        "    print(\"=== Load JSON ===\")\n",
        "    with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if isinstance(data, dict) and \"items\" in data:\n",
        "        data = data[\"items\"]\n",
        "\n",
        "    print(f\"- ë¡œë“œëœ ì‹œì„¤ ìˆ˜: {len(data)}\")\n",
        "\n",
        "    # ë¬¸ì¥í™” ë° ë©”íƒ€ë°ì´í„° êµ¬ì„±\n",
        "    documents, metas, ids = [], [], []\n",
        "    for i, item in enumerate(data):\n",
        "        doc = make_text(item)\n",
        "        meta = _std_facility_meta(item)\n",
        "        documents.append(doc)\n",
        "        metas.append(meta)\n",
        "        ids.append(meta[\"id\"] or f\"fac_{i}\")\n",
        "\n",
        "    print(\"=== Embedding ===\")\n",
        "    embedder = FacilityEmbeddingFunction()\n",
        "    embeddings = embedder(documents)\n",
        "    print(f\"- ì„ë² ë”© ì™„ë£Œ ({len(embeddings)}ê°œ)\")\n",
        "\n",
        "    print(\"=== Save to ChromaDB ===\")\n",
        "    client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
        "    collection = client.get_or_create_collection(COLLECTION_NAME)\n",
        "    collection.upsert(ids=ids, documents=documents, metadatas=metas, embeddings=embeddings)\n",
        "\n",
        "    print(f\"âœ… ì™„ë£Œ! ì´ {len(documents)}ê°œ ì‹œì„¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"ğŸ“‚ DB ê²½ë¡œ: {CHROMA_PATH}\")\n",
        "    print(f\"ğŸ“˜ ì»¬ë ‰ì…˜ ì´ë¦„: {COLLECTION_NAME}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    embed_facilities_to_chroma()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì¸ì²œê³µí•­ ì¶œë°œ/ë„ì°© í•­ê³µí¸ ìŠ¤ì¼€ì¥´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°**"
      ],
      "metadata": {
        "id": "OQo2qh0OGl6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUBVIEW_URL   = \"https://www.airport.kr/ap_ko/869/subview.do\"\n",
        "DEP_LIST_URL  = \"https://www.airport.kr/dep/ap_ko/getDepPasSchList.do\"\n",
        "ARR_LIST_URL  = \"https://www.airport.kr/arr/ap_ko/getArrPasSchList.do\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
        "    \"Referer\": SUBVIEW_URL,\n",
        "    \"Origin\": \"https://www.airport.kr\",\n",
        "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
        "}\n",
        "\n",
        "KST = timezone(timedelta(hours=9))  # í•œêµ­ íƒ€ì„ì¡´\n",
        "\n",
        "# ===== ê³µë™ ìœ í‹¸ =====\n",
        "def ymd(d: date) -> str:\n",
        "    return d.strftime(\"%Y%m%d\")\n",
        "\n",
        "def flatten(d, out=None, prefix=\"\"):\n",
        "    if out is None: out = {}\n",
        "    for k, v in d.items():\n",
        "        kk = f\"{prefix}{k}\" if not prefix else f\"{prefix}.{k}\"\n",
        "        if isinstance(v, dict):\n",
        "            flatten(v, out, kk)\n",
        "        else:\n",
        "            out[kk] = v\n",
        "    return out\n",
        "\n",
        "def get_layout_token(session: requests.Session) -> str:\n",
        "    r = session.get(SUBVIEW_URL, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    html = r.text\n",
        "    m = re.search(r'name=[\"\\']layout[\"\\']\\s+value=[\"\\']([0-9a-fA-F]+)[\"\\']', html)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    m = re.search(r'layout\\s*[:=]\\s*[\"\\']([0-9a-fA-F]+)[\"\\']', html)\n",
        "    return m.group(1) if m else \"\"\n",
        "\n",
        "def post_list(session: requests.Session, url: str, payload: dict):\n",
        "    r = session.post(url, headers=HEADERS, data=payload, timeout=25)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def _build_params(day: str, layout: str, start_hhmm: str, end_hhmm: str, row: int):\n",
        "    return {\n",
        "        \"intg\": \"\",\n",
        "        \"keyWord\": \"\",\n",
        "        \"curDate\": day,\n",
        "        \"startTime\": start_hhmm,\n",
        "        \"airPort\": \"\",\n",
        "        \"endTime\": end_hhmm,\n",
        "        \"todayDate\": day,\n",
        "        \"tomorrowDate\": (datetime.strptime(day, \"%Y%m%d\") + timedelta(days=1)).strftime(\"%Y%m%d\"),\n",
        "        \"todayTime\": \"0000\",\n",
        "        \"curStime\": start_hhmm,\n",
        "        \"curEtime\": end_hhmm,\n",
        "        \"layout\": layout,\n",
        "        \"siteId\": \"ap_ko\",\n",
        "        \"langSe\": \"ko\",\n",
        "        \"scheduleListLength\": \"\",\n",
        "        \"termId\": \"\",\n",
        "        \"daySel\": day,\n",
        "        \"fromTime\": start_hhmm,\n",
        "        \"toTime\": end_hhmm,\n",
        "        \"airport\": \"\",\n",
        "        \"airline\": \"\",\n",
        "        \"airplane\": \"\",\n",
        "        \"page\": \"1\",\n",
        "        \"row\": str(row),\n",
        "    }\n",
        "\n",
        "def to_iso_pair(ymd_str: str, hhmm: str):\n",
        "    if not ymd_str or not hhmm or not hhmm.strip():\n",
        "        return None, None\n",
        "    try:\n",
        "        dt = datetime.strptime(ymd_str + hhmm, \"%Y%m%d%H%M\").replace(tzinfo=KST)\n",
        "        return dt.isoformat(), dt.astimezone(timezone.utc).isoformat()\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def parse_codeshares(raw: str):\n",
        "    if not raw:\n",
        "        return []\n",
        "    return [x.strip() for x in raw.split(\",\") if x.strip()]\n",
        "\n",
        "# ===== í¸ëª… ì •ê·œí™” + ì¤‘ë³µí‚¤ =====\n",
        "def _norm_flight_no_from_row(row: dict) -> str:\n",
        "    carrier = (row.get(\"flightCarrier\") or \"\").upper().strip()\n",
        "    fnum    = (row.get(\"fnumber\") or \"\").upper().strip()\n",
        "    raw     = re.sub(r\"[^A-Z0-9]\", \"\", f\"{carrier}{fnum}\")\n",
        "    m = re.match(r\"^([A-Z]{2})\\1(\\d+)$\", raw)  # KEKE5879 â†’ KE5879\n",
        "    return m.group(1) + m.group(2) if m else raw\n",
        "\n",
        "def _dest_key(row: dict) -> str:\n",
        "    return ((row.get(\"airportName1Ko\") or row.get(\"airportName1En\") or \"\")\n",
        "            .upper().replace(\" \", \"\").replace(\"/\", \"\"))\n",
        "\n",
        "def _dep_key(row: dict) -> tuple:\n",
        "    sdate = (row.get(\"sdate\") or \"\").strip()\n",
        "    fno   = _norm_flight_no_from_row(row)\n",
        "    dest  = _dest_key(row)\n",
        "    stime = (row.get(\"stime\") or \"\").strip()\n",
        "    term  = (row.get(\"terminal\") or \"\").upper().strip()\n",
        "    return (\"DEP\", sdate, fno, dest, stime, term)\n",
        "\n",
        "def _orig_key(row: dict) -> str:\n",
        "    return ((row.get(\"airportName1Ko\") or row.get(\"airportName1En\") or \"\")\n",
        "            .upper().replace(\" \", \"\").replace(\"/\", \"\"))\n",
        "\n",
        "def _arr_key(row: dict) -> tuple:\n",
        "\n",
        "    sdate = (row.get(\"sdate\") or \"\").strip()\n",
        "    fno   = _norm_flight_no_from_row(row)\n",
        "    orig  = _orig_key(row)\n",
        "    stime = (row.get(\"stime\") or \"\").strip()\n",
        "    term  = (row.get(\"terminal\") or \"\").upper().strip()\n",
        "    return (\"ARR\", sdate, fno, orig, stime, term)\n",
        "\n",
        "def _dedup_dep_rows(rows: list) -> list:\n",
        "    seen, out = set(), []\n",
        "    for r in rows:\n",
        "        key = _dep_key(r)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append(r)\n",
        "    return out\n",
        "\n",
        "def _dedup_arr_rows(rows: list) -> list:\n",
        "    seen, out = set(), []\n",
        "    for r in rows:\n",
        "        key = _arr_key(r)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append(r)\n",
        "    return out\n",
        "\n",
        "# ===== ì¶œë°œí¸ ìˆ˜ì§‘ =====\n",
        "def fetch_dep_day_once(session: requests.Session, d: date, layout: str, row=800):\n",
        "    day = ymd(d)\n",
        "    params_common = _build_params(day, layout, \"0000\", \"2359\", row)\n",
        "    items_all, seen = [], set()\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = dict(params_common); params[\"page\"] = str(page)\n",
        "        j = post_list(session, DEP_LIST_URL, params)\n",
        "        items = j.get(\"scheduleList\") or []\n",
        "        if not items:\n",
        "            break\n",
        "        for it in items:\n",
        "            key = _dep_key(it)\n",
        "            if key in seen:\n",
        "                continue\n",
        "            seen.add(key)\n",
        "            items_all.append(it)\n",
        "        total = int(j.get(\"scheduleSize\", 0) or 0)\n",
        "        if page * row >= total:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(0.1)\n",
        "    return items_all\n",
        "\n",
        "def fetch_dep_day_hourly_fallback(session: requests.Session, d: date, layout: str, row=800):\n",
        "    day = ymd(d)\n",
        "    items_all, seen = [], set()\n",
        "    for hh in range(24):\n",
        "        start, end = f\"{hh:02d}00\", f\"{hh:02d}59\"\n",
        "        params_common = _build_params(day, layout, start, end, row)\n",
        "        page = 1\n",
        "        while True:\n",
        "            params = dict(params_common); params[\"page\"] = str(page)\n",
        "            j = post_list(session, DEP_LIST_URL, params)\n",
        "            items = j.get(\"scheduleList\") or []\n",
        "            if not items:\n",
        "                break\n",
        "            for it in items:\n",
        "                key = _dep_key(it)\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                items_all.append(it)\n",
        "            total = int(j.get(\"scheduleSize\", 0) or 0)\n",
        "            if page * row >= total:\n",
        "                break\n",
        "            page += 1\n",
        "            time.sleep(0.06)\n",
        "        time.sleep(0.06)\n",
        "    return items_all\n",
        "\n",
        "# ===== ë„ì°©í¸ ìˆ˜ì§‘ =====\n",
        "def fetch_arr_day_once(session: requests.Session, d: date, layout: str, row=800):\n",
        "    day = ymd(d)\n",
        "    params_common = _build_params(day, layout, \"0000\", \"2359\", row)\n",
        "    items_all, seen = [], set()\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = dict(params_common); params[\"page\"] = str(page)\n",
        "        j = post_list(session, ARR_LIST_URL, params)\n",
        "        items = j.get(\"scheduleList\") or []\n",
        "        if not items:\n",
        "            break\n",
        "        for it in items:\n",
        "            key = _arr_key(it)\n",
        "            if key in seen:\n",
        "                continue\n",
        "            seen.add(key)\n",
        "            items_all.append(it)\n",
        "        total = int(j.get(\"scheduleSize\", 0) or 0)\n",
        "        if page * row >= total:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(0.1)\n",
        "    return items_all\n",
        "\n",
        "def fetch_arr_day_hourly_fallback(session: requests.Session, d: date, layout: str, row=800):\n",
        "    day = ymd(d)\n",
        "    items_all, seen = [], set()\n",
        "    for hh in range(24):\n",
        "        start, end = f\"{hh:02d}00\", f\"{hh:02d}59\"\n",
        "        params_common = _build_params(day, layout, start, end, row)\n",
        "        page = 1\n",
        "        while True:\n",
        "            params = dict(params_common); params[\"page\"] = str(page)\n",
        "            j = post_list(session, ARR_LIST_URL, params)\n",
        "            items = j.get(\"scheduleList\") or []\n",
        "            if not items:\n",
        "                break\n",
        "            for it in items:\n",
        "                key = _arr_key(it)\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                items_all.append(it)\n",
        "            total = int(j.get(\"scheduleSize\", 0) or 0)\n",
        "            if page * row >= total:\n",
        "                break\n",
        "            page += 1\n",
        "            time.sleep(0.06)\n",
        "        time.sleep(0.06)\n",
        "    return items_all\n",
        "\n",
        "# ===== ë ˆì½”ë“œ ì •ê·œí™” (RAGìš©) =====\n",
        "def to_record_departure(row: dict):\n",
        "\n",
        "    sdate = row.get(\"sdate\")\n",
        "    stime = row.get(\"stime\")\n",
        "    etime = row.get(\"etime\")\n",
        "\n",
        "    sched_local, sched_utc = to_iso_pair(sdate, stime)\n",
        "    upd_local, upd_utc     = to_iso_pair(sdate, etime)\n",
        "\n",
        "    airline_code = (row.get(\"flightCarrier\") or \"\").strip().upper()\n",
        "    fnum         = (row.get(\"fnumber\") or \"\").strip().upper()\n",
        "    flight_no    = _norm_flight_no_from_row(row)\n",
        "\n",
        "    codeshare_role = (row.get(\"codeshareFlight\") or \"\").strip()\n",
        "    is_master = codeshare_role.lower() == \"master\"\n",
        "\n",
        "    dest_key = _dest_key(row)\n",
        "    term     = (row.get(\"terminal\") or \"\").upper().strip()\n",
        "    doc_id   = f\"DEP:{sdate}:{flight_no}:{stime}:{dest_key}:{term}\"\n",
        "\n",
        "    return {\n",
        "        \"doc_id\": doc_id,\n",
        "        \"arr_or_dep\": \"DEP\",\n",
        "        \"date\": sdate,  # YYYYMMDD\n",
        "        \"flight_no\": flight_no,\n",
        "        \"airline_code\": airline_code,\n",
        "        \"airline_name\": row.get(\"linename\"),\n",
        "        \"route\": {\n",
        "            \"destination_ko\": row.get(\"airportName1Ko\"),\n",
        "            \"destination_en\": row.get(\"airportName1En\"),\n",
        "            \"iata\": row.get(\"p1code\"),\n",
        "        },\n",
        "        \"terminal\": row.get(\"terminal\"),\n",
        "        \"dep_gate\": row.get(\"gatenumber\"),\n",
        "        \"checkin_range\": row.get(\"chkinrange\"),\n",
        "        \"status\": row.get(\"stattxt\"),\n",
        "        \"codeshare_role\": codeshare_role,\n",
        "        \"is_master\": is_master,\n",
        "        \"codeshares\": parse_codeshares(row.get(\"codeshare\")),\n",
        "        \"times\": {\n",
        "            \"scheduled_local\": sched_local,\n",
        "            \"updated_local\":   upd_local,\n",
        "            \"scheduled_utc\":   sched_utc,\n",
        "            \"updated_utc\":     upd_utc,\n",
        "        },\n",
        "        \"text_ko\": f\"{sdate} {row.get('linename') or ''} {flight_no} {row.get('airportName1Ko') or ''} ì¶œë°œ \"\n",
        "                   f\"í„°ë¯¸ë„ {row.get('terminal') or ''} íƒ‘ìŠ¹êµ¬ {row.get('gatenumber') or ''} \"\n",
        "                   f\"ì²´í¬ì¸ {row.get('chkinrange') or ''} ìƒíƒœ:{row.get('stattxt') or ''}\".strip(),\n",
        "        \"last_seen_at\": datetime.now(KST).isoformat(),\n",
        "    }\n",
        "\n",
        "def to_record_arrival(row: dict):\n",
        "\n",
        "    sdate = row.get(\"sdate\")\n",
        "    stime = row.get(\"stime\")\n",
        "    etime = row.get(\"etime\")\n",
        "\n",
        "    sched_local, sched_utc = to_iso_pair(sdate, stime)\n",
        "    upd_local, upd_utc     = to_iso_pair(sdate, etime)\n",
        "\n",
        "    airline_code = (row.get(\"flightCarrier\") or \"\").strip().upper()\n",
        "    fnum         = (row.get(\"fnumber\") or \"\").strip().upper()\n",
        "    flight_no    = _norm_flight_no_from_row(row)\n",
        "\n",
        "    codeshare_role = (row.get(\"codeshareFlight\") or \"\").strip()\n",
        "    is_master = codeshare_role.lower() == \"master\"\n",
        "\n",
        "    orig_key = _orig_key(row)\n",
        "    term     = (row.get(\"terminal\") or \"\").upper().strip()\n",
        "    doc_id   = f\"ARR:{sdate}:{flight_no}:{stime}:{orig_key}:{term}\"\n",
        "\n",
        "    return {\n",
        "        \"doc_id\": doc_id,\n",
        "        \"arr_or_dep\": \"ARR\",\n",
        "        \"date\": sdate,\n",
        "        \"flight_no\": flight_no,\n",
        "        \"airline_code\": airline_code,\n",
        "        \"airline_name\": row.get(\"linename\"),\n",
        "        \"route\": {\n",
        "            \"origin_ko\": row.get(\"airportName1Ko\"),\n",
        "            \"origin_en\": row.get(\"airportName1En\"),\n",
        "            \"iata\": row.get(\"p1code\"),\n",
        "        },\n",
        "        \"terminal\": row.get(\"terminal\"),\n",
        "        \"arr_gate\": row.get(\"gatenumber\"),\n",
        "        \"baggage_carousel\": row.get(\"carousel\"),   # ìˆ˜í•˜ë¬¼ìˆ˜ì·¨ëŒ€\n",
        "        \"arr_exit\": row.get(\"exitnumber\"),         # ì…êµ­ì¥ ì¶œêµ¬\n",
        "        \"status\": row.get(\"stattxt\"),\n",
        "        \"codeshare_role\": codeshare_role,\n",
        "        \"is_master\": is_master,\n",
        "        \"codeshares\": parse_codeshares(row.get(\"codeshare\")),\n",
        "        \"times\": {\n",
        "            \"scheduled_local\": sched_local,\n",
        "            \"updated_local\":   upd_local,\n",
        "            \"scheduled_utc\":   sched_utc,\n",
        "            \"updated_utc\":     upd_utc,\n",
        "        },\n",
        "        \"text_ko\": f\"{sdate} {row.get('linename') or ''} {flight_no} {row.get('airportName1Ko') or ''} ë„ì°© \"\n",
        "                   f\"í„°ë¯¸ë„ {row.get('terminal') or ''} ë„ì°©ê²Œì´íŠ¸ {row.get('gatenumber') or ''} \"\n",
        "                   f\"ìˆ˜í•˜ë¬¼ìˆ˜ì·¨ëŒ€ {row.get('carousel') or ''} ì¶œêµ¬ {row.get('exitnumber') or ''} \"\n",
        "                   f\"ìƒíƒœ:{row.get('stattxt') or ''}\".strip(),\n",
        "        \"last_seen_at\": datetime.now(KST).isoformat(),\n",
        "    }\n",
        "\n",
        "# ===== ì €ì¥ ìœ í‹¸ =====\n",
        "def save_jsonl(records, out_path: str):\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in records:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def save_csv_departures(items, out_path=\"incheon_departures.csv\"):\n",
        "    items = _dedup_dep_rows(items)\n",
        "    flat = [flatten(x) for x in items]\n",
        "    if not flat:\n",
        "        headers = [\"ë‚ ì§œ\",\"ì¶œë°œì‹œê°„\",\"ë³€ê²½ì‹œê°„\",\"ëª©ì ì§€\",\"í•­ê³µì‚¬\",\"ìš´í•­í¸ëª…\",\"í„°ë¯¸ë„\",\"ì²´í¬ì¸ ì¹´ìš´í„°\",\"íƒ‘ìŠ¹êµ¬\",\"ì¶œë°œí˜„í™©\",\"CODESHARE\"]\n",
        "        with open(out_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "            csv.DictWriter(f, fieldnames=headers).writeheader()\n",
        "        return\n",
        "    mapping = {\n",
        "        \"sdate\": \"ë‚ ì§œ\", \"stime\": \"ì¶œë°œì‹œê°„\", \"etime\": \"ë³€ê²½ì‹œê°„\",\n",
        "        \"airportName1Ko\": \"ëª©ì ì§€\", \"linename\": \"í•­ê³µì‚¬\", \"fnumber\": \"ìš´í•­í¸ëª…\",\n",
        "        \"terminal\": \"í„°ë¯¸ë„\", \"chkinrange\": \"ì²´í¬ì¸ ì¹´ìš´í„°\", \"gatenumber\": \"íƒ‘ìŠ¹êµ¬\",\n",
        "        \"stattxt\": \"ì¶œë°œí˜„í™©\", \"codeshareFlight\": \"CODESHARE\",\n",
        "    }\n",
        "    ordered = list(mapping.values())\n",
        "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=ordered); w.writeheader()\n",
        "        for row in flat:\n",
        "            w.writerow({dst: row.get(src, \"\") for src, dst in mapping.items()})\n",
        "\n",
        "def save_csv_arrivals(items, out_path=\"incheon_arrivals.csv\"):\n",
        "    items = _dedup_arr_rows(items)\n",
        "    flat = [flatten(x) for x in items]\n",
        "    if not flat:\n",
        "        headers = [\"ë‚ ì§œ\",\"ë„ì°©ì‹œê°„\",\"ë³€ê²½ì‹œê°„\",\"ì¶œë°œì§€\",\"í•­ê³µì‚¬\",\"ìš´í•­í¸ëª…\",\"í„°ë¯¸ë„\",\"ë„ì°©ê²Œì´íŠ¸\",\"ìˆ˜í•˜ë¬¼ìˆ˜ì·¨ëŒ€\",\"ì…êµ­ì¥ ì¶œêµ¬\",\"ë„ì°©í˜„í™©\",\"CODESHARE\"]\n",
        "        with open(out_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "            csv.DictWriter(f, fieldnames=headers).writeheader()\n",
        "        return\n",
        "    mapping = {\n",
        "        \"sdate\": \"ë‚ ì§œ\", \"stime\": \"ë„ì°©ì‹œê°„\", \"etime\": \"ë³€ê²½ì‹œê°„\",\n",
        "        \"airportName1Ko\": \"ì¶œë°œì§€\", \"linename\": \"í•­ê³µì‚¬\", \"fnumber\": \"ìš´í•­í¸ëª…\",\n",
        "        \"terminal\": \"í„°ë¯¸ë„\", \"gatenumber\": \"ë„ì°©ê²Œì´íŠ¸\", \"carousel\": \"ìˆ˜í•˜ë¬¼ìˆ˜ì·¨ëŒ€\",\n",
        "        \"exitnumber\": \"ì…êµ­ì¥ ì¶œêµ¬\", \"stattxt\": \"ë„ì°©í˜„í™©\", \"codeshareFlight\": \"CODESHARE\",\n",
        "    }\n",
        "    ordered = list(mapping.values())\n",
        "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=ordered); w.writeheader()\n",
        "        for row in flat:\n",
        "            w.writerow({dst: row.get(src, \"\") for src, dst in mapping.items()})"
      ],
      "metadata": {
        "id": "G6eYn-n0B-WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== ì‹¤í–‰ë¶€ =====\n",
        "if __name__ == \"__main__\":\n",
        "    # KST ê¸°ì¤€ ê¸°ë³¸ ê¸°ê°„: ì˜¤ëŠ˜ ~ ì˜¤ëŠ˜+6ì¼\n",
        "    today_kst = datetime.now(KST).date()\n",
        "\n",
        "    # ===== ì—¬ê¸°ë§Œ ìˆ˜ì • =====\n",
        "    START_DATE    = None  # ì˜ˆ: date(2025, 10, 5)\n",
        "    END_DATE      = None  # ì˜ˆ: date(2025, 10, 8)\n",
        "    DO_DEPARTURES = True\n",
        "    DO_ARRIVALS   = True\n",
        "    # ======================\n",
        "\n",
        "    start = START_DATE or today_kst\n",
        "    end   = END_DATE   or (today_kst + timedelta(days=6))\n",
        "\n",
        "    print(\"=== ì¸ì²œê³µí•­ ìŠ¤ì¼€ì¤„ ìˆ˜ì§‘ ë° RAG ì „ì²˜ë¦¬ ===\")\n",
        "    print(f\"ê¸°ê°„(KST): {start} ~ {end}\")\n",
        "    print(f\"- ì¶œë°œí¸: {DO_DEPARTURES}, ë„ì°©í¸: {DO_ARRIVALS}\")\n",
        "\n",
        "    s = requests.Session(); s.headers.update(HEADERS)\n",
        "    layout = get_layout_token(s)\n",
        "\n",
        "    # ===== ì¶œë°œ =====\n",
        "    if DO_DEPARTURES:\n",
        "        all_dep_raw = []\n",
        "        cur = start\n",
        "        while cur <= end:\n",
        "            print(f\"[ì¶œë°œ {cur}] í•˜ë£¨ ë‹¨ì¼ ìš”ì²­\")\n",
        "            rows = fetch_dep_day_once(s, cur, layout=layout, row=800)\n",
        "            if not rows:\n",
        "                print(f\"[ì¶œë°œ {cur}] ë‹¨ì¼ìš”ì²­ ì‹¤íŒ¨ â†’ ì‹œê°„ë³„ í´ë°±\")\n",
        "                rows = fetch_dep_day_hourly_fallback(s, cur, layout=layout, row=800)\n",
        "            print(f\"[ì¶œë°œ {cur}] {len(rows)} ê±´\")\n",
        "            all_dep_raw.extend(rows)\n",
        "            cur += timedelta(days=1)\n",
        "            time.sleep(0.0)\n",
        "\n",
        "        # RAG ì •ê·œí™” + upsertìš© doc_id ê¸°ì¤€ ì¤‘ë³µ ì œê±°\n",
        "        dep_records = {}\n",
        "        for r in all_dep_raw:\n",
        "            rec = to_record_departure(flatten(r))\n",
        "            dep_records[rec[\"doc_id\"]] = rec\n",
        "        save_jsonl(list(dep_records.values()), \"incheon_departures_rag.jsonl\")\n",
        "        # (ì„ íƒ) CSV ìš”ì•½ ì €ì¥\n",
        "        save_csv_departures(all_dep_raw, \"incheon_departures.csv\")\n",
        "        print(\"ì¶œë°œí¸ ì €ì¥: incheon_departures_rag.jsonl / incheon_departures.csv\")\n",
        "\n",
        "    # ===== ë„ì°© =====\n",
        "    if DO_ARRIVALS:\n",
        "        all_arr_raw = []\n",
        "        cur = start\n",
        "        while cur <= end:\n",
        "            print(f\"[ë„ì°© {cur}] í•˜ë£¨ ë‹¨ì¼ ìš”ì²­\")\n",
        "            rows = fetch_arr_day_once(s, cur, layout=layout, row=800)\n",
        "            if not rows:\n",
        "                print(f\"[ë„ì°© {cur}] ë‹¨ì¼ìš”ì²­ ì‹¤íŒ¨ â†’ ì‹œê°„ë³„ í´ë°±\")\n",
        "                rows = fetch_arr_day_hourly_fallback(s, cur, layout=layout, row=800)\n",
        "            print(f\"[ë„ì°© {cur}] {len(rows)} ê±´\")\n",
        "            all_arr_raw.extend(rows)\n",
        "            cur += timedelta(days=1)\n",
        "            time.sleep(0.0)\n",
        "\n",
        "        arr_records = {}\n",
        "        for r in all_arr_raw:\n",
        "            rec = to_record_arrival(flatten(r))\n",
        "            arr_records[rec[\"doc_id\"]] = rec\n",
        "        save_jsonl(list(arr_records.values()), \"incheon_arrivals_rag.jsonl\")\n",
        "        save_csv_arrivals(all_arr_raw, \"incheon_arrivals.csv\")\n",
        "        print(\"ë„ì°©í¸ ì €ì¥: incheon_arrivals_rag.jsonl / incheon_arrivals.csv\")\n",
        "\n",
        "    print(\"ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "4zs4UWmMGtjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **í•­ê³µí¸ ì²­í‚¹ ë° ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "V1PaepGPG6YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== ì„¤ì • =====\n",
        "DEPART_CSV = \"/content/incheon_departures.csv\"\n",
        "ARRIVE_CSV = \"/content/incheon_arrivals.csv\"\n",
        "CHROMA_PATH = \"/content/chroma_flights\"\n",
        "COLLECTION_NAME = \"flights\"\n",
        "MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
        "\n",
        "\n",
        "# ===== ì„ë² ë”© ìœ í‹¸ =====\n",
        "class FlightEmbeddingFunction:\n",
        "    def __init__(self, model_name=MODEL_NAME):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def preprocess(self, text: str) -> str:\n",
        "        text = text.lower().strip()\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "        text = re.sub(r\"ì œ\\s*([12])\", r\"ì œ\\1\", text)  # ì œ 1 â†’ ì œ1\n",
        "        text = text.replace(\"ì—¬ê° í„°ë¯¸ë„\", \"ì—¬ê°í„°ë¯¸ë„\")\n",
        "        return text\n",
        "\n",
        "    def __call__(self, texts):\n",
        "        pre = [self.preprocess(t) for t in texts]\n",
        "        return self.model.encode(pre, normalize_embeddings=True)\n",
        "\n",
        "\n",
        "# ===== CSV ë¡œë“œ =====\n",
        "def load_flight_data() -> pd.DataFrame:\n",
        "    dfs = []\n",
        "    if os.path.exists(DEPART_CSV):\n",
        "        df = pd.read_csv(DEPART_CSV)\n",
        "        df[\"arr_or_dep\"] = \"ì¶œë°œ\"\n",
        "        dfs.append(df)\n",
        "\n",
        "    if os.path.exists(ARRIVE_CSV):\n",
        "        df = pd.read_csv(ARRIVE_CSV)\n",
        "        df[\"arr_or_dep\"] = \"ë„ì°©\"\n",
        "        # ì»¬ëŸ¼ëª… í†µì¼\n",
        "        df.rename(columns={\n",
        "            \"ë„ì°©ê²Œì´íŠ¸\": \"íƒ‘ìŠ¹êµ¬\",\n",
        "            \"ë„ì°©í˜„í™©\": \"ë„ì°©í˜„í™©\",\n",
        "            \"ë„ì°©ì‹œê°„\": \"ë„ì°©ì‹œê°„\"\n",
        "        }, inplace=True)\n",
        "        dfs.append(df)\n",
        "\n",
        "    if not dfs:\n",
        "        raise FileNotFoundError(\"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    df_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # ì‹œê°„ í¬ë§· ì •ê·œí™”\n",
        "    def normalize_time(t):\n",
        "        if pd.isna(t): return \"-\"\n",
        "        t = str(t).strip()\n",
        "        t = re.sub(r\"(\\.0|:00)$\", \"\", t)\n",
        "        return t\n",
        "\n",
        "    for col in [\"ì¶œë°œì‹œê°„\", \"ë„ì°©ì‹œê°„\", \"ë³€ê²½ì‹œê°„\"]:\n",
        "        if col in df_all.columns:\n",
        "            df_all[col] = df_all[col].apply(normalize_time)\n",
        "\n",
        "    print(f\"âœ… ì´ {len(df_all)}ê°œ í•­ê³µí¸ ë¡œë“œ ì™„ë£Œ ({df_all['arr_or_dep'].value_counts().to_dict()})\")\n",
        "    return df_all\n",
        "\n",
        "\n",
        "# ===== ë¬¸ì¥í™” =====\n",
        "def make_passage(row, label=\"ì¶œë°œ\"):\n",
        "    fno = row.get(\"ìš´í•­í¸ëª…\") or \"-\"\n",
        "    airline = row.get(\"í•­ê³µì‚¬\") or \"-\"\n",
        "    terminal = row.get(\"í„°ë¯¸ë„\") or \"-\"\n",
        "    code_share = row.get(\"CODESHARE\") or \"\"\n",
        "    date = row.get(\"ë‚ ì§œ\") or \"-\"\n",
        "\n",
        "    sched = row.get(\"ì¶œë°œì‹œê°„\") if label == \"ì¶œë°œ\" else row.get(\"ë„ì°©ì‹œê°„\")\n",
        "    changed = row.get(\"ë³€ê²½ì‹œê°„\")\n",
        "    if changed and str(changed).strip() and changed != sched:\n",
        "        time_text = f\"{sched} â†’ {changed} (ë³€ê²½ë¨)\"\n",
        "    else:\n",
        "        time_text = sched or \"-\"\n",
        "\n",
        "    if label == \"ì¶œë°œ\":\n",
        "        dest = row.get(\"ëª©ì ì§€\") or \"-\"\n",
        "        gate = row.get(\"íƒ‘ìŠ¹êµ¬\") or \"-\"\n",
        "        counter = row.get(\"ì²´í¬ì¸ ì¹´ìš´í„°\") or \"-\"\n",
        "        status = row.get(\"ì¶œë°œí˜„í™©\") or row.get(\"ìƒíƒœ\") or \"-\"\n",
        "        txt = (\n",
        "            f\"[ì¶œë°œ] {airline} {fno}í¸ì€ {date} \"\n",
        "            f\"{terminal} í„°ë¯¸ë„ {gate} ê²Œì´íŠ¸ì—ì„œ {time_text} ì¶œë°œ ì˜ˆì •ì…ë‹ˆë‹¤. \"\n",
        "            f\"ì²´í¬ì¸ì€ {counter}ì—ì„œ ì§„í–‰ë˜ë©°, í˜„ì¬ ìƒíƒœëŠ” '{status}'ì…ë‹ˆë‹¤. \"\n",
        "            f\"ëª©ì ì§€ëŠ” {dest}ì…ë‹ˆë‹¤.\"\n",
        "        )\n",
        "    else:\n",
        "        origin = row.get(\"ì¶œë°œì§€\") or \"-\"\n",
        "        gate = row.get(\"íƒ‘ìŠ¹êµ¬\") or \"-\"\n",
        "        belt = row.get(\"ìˆ˜í•˜ë¬¼ìˆ˜ì·¨ëŒ€\") or \"-\"\n",
        "        exit_gate = row.get(\"ì…êµ­ì¥ ì¶œêµ¬\") or \"-\"\n",
        "        status = row.get(\"ë„ì°©í˜„í™©\") or row.get(\"ìƒíƒœ\") or \"-\"\n",
        "        txt = (\n",
        "            f\"[ë„ì°©] {airline} {fno}í¸ì€ {date} \"\n",
        "            f\"{terminal} í„°ë¯¸ë„ {gate} ê²Œì´íŠ¸ë¡œ {time_text} ë„ì°© ì˜ˆì •ì…ë‹ˆë‹¤. \"\n",
        "            f\"ì¶œë°œì§€ëŠ” {origin}ì´ë©°, ìˆ˜í•˜ë¬¼ì€ {belt}ë²ˆ ìˆ˜ì·¨ëŒ€, ì…êµ­ì¥ì€ {exit_gate} ì¶œêµ¬ë¥¼ ì´ìš©í•©ë‹ˆë‹¤. \"\n",
        "            f\"í˜„ì¬ ìƒíƒœëŠ” '{status}'ì…ë‹ˆë‹¤.\"\n",
        "        )\n",
        "\n",
        "    if code_share and str(code_share).strip() not in (\"-\", \"\"):\n",
        "        txt += f\" (ê³µë™ìš´í•­í¸: {code_share})\"\n",
        "    return txt\n",
        "\n",
        "\n",
        "# ===== ë©”ì¸ =====\n",
        "def embed_flights_to_chroma():\n",
        "    print(\"=== Step 1: Load CSV ===\")\n",
        "    df = load_flight_data()\n",
        "\n",
        "    print(\"=== Step 2: Make passages ===\")\n",
        "    documents, metadatas, ids = [], [], []\n",
        "    for i, row in df.iterrows():\n",
        "        r = row.to_dict()\n",
        "        label = r.get(\"arr_or_dep\", \"ì¶œë°œ\")\n",
        "        text = make_passage(r, label)\n",
        "        documents.append(text)\n",
        "        metadatas.append(r)\n",
        "        fid = f\"{label}_{r.get('ìš´í•­í¸ëª…','UNK')}_{r.get('ë‚ ì§œ','NA')}_{i}\"\n",
        "        ids.append(fid)\n",
        "\n",
        "    print(f\"ì´ {len(documents)}ê°œ ë¬¸ì¥ ë³€í™˜ ì™„ë£Œ\")\n",
        "\n",
        "    print(\"=== Step 3: Embedding ===\")\n",
        "    embedder = FlightEmbeddingFunction()\n",
        "    embeddings = embedder(documents)\n",
        "    print(f\"- ì„ë² ë”© ì™„ë£Œ ({len(embeddings)}ê°œ)\")\n",
        "\n",
        "    print(\"=== Step 4: Save to ChromaDB (Batch) ===\")\n",
        "    client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
        "    collection = client.get_or_create_collection(COLLECTION_NAME)\n",
        "\n",
        "    BATCH_SIZE = 5000\n",
        "    total = len(documents)\n",
        "    for i in range(0, total, BATCH_SIZE):\n",
        "        end = min(i + BATCH_SIZE, total)\n",
        "        print(f\"  - Upserting {i} ~ {end} / {total}\")\n",
        "        collection.upsert(\n",
        "            ids=ids[i:end],\n",
        "            documents=documents[i:end],\n",
        "            metadatas=metadatas[i:end],\n",
        "            embeddings=embeddings[i:end],\n",
        "        )\n",
        "\n",
        "    print(f\"âœ… ì™„ë£Œ! ì´ {len(documents)}ê°œ í•­ê³µí¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"ğŸ“‚ DB ê²½ë¡œ: {CHROMA_PATH}\")\n",
        "    print(f\"ğŸ“˜ ì»¬ë ‰ì…˜ ì´ë¦„: {COLLECTION_NAME}\")\n",
        "    print(\"ğŸ§® ì¸ë±ìŠ¤ í†µê³„:\", collection.count())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    embed_flights_to_chroma()\n"
      ],
      "metadata": {
        "id": "fVn1ZVO9R2lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ë²„ìŠ¤ ì •ë³´ í¬ë¡¤ë§ ë° ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "7YYIAsuZkMd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ì¿ í‚¤ ì…ë ¥\n",
        "# =========================================================\n",
        "RAW_COOKIE = \"\"\"\n",
        "WMONID=XXX;\n",
        "PCID=YYY;\n",
        "JSESSIONID=ZZZ;\n",
        "\"\"\"\n",
        "COOKIE = RAW_COOKIE.replace(\"\\n\", \"\").strip()\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\",\n",
        "    \"Accept\": \"text/html, */*; q=0.01\",\n",
        "    \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
        "    \"Origin\": \"https://www.airport.kr\",\n",
        "    \"Referer\": \"https://www.airport.kr/ap_ko/976/subview.do\",\n",
        "    \"Cookie\": COOKIE\n",
        "}\n",
        "\n",
        "LIST_URL = \"https://www.airport.kr/ap_ko/976/subview.do?routeArea={area}\"\n",
        "DETAIL_URL = \"https://www.airport.kr/bus/ap_ko/busInfoDetail.do\"\n",
        "\n",
        "# =========================================================\n",
        "# 1) ì§€ì—­ ì½”ë“œ\n",
        "# =========================================================\n",
        "REGIONS = {\n",
        "    1: \"ì„œìš¸\",\n",
        "    2: \"ê²½ê¸°ë„\",\n",
        "    3: \"ì¸ì²œ\",\n",
        "    4: \"ê°•ì›ë„\",\n",
        "    5: \"ì¶©ì²­ë„\",\n",
        "    6: \"ê²½ìƒë„\",\n",
        "    7: \"ì „ë¼ë„\",\n",
        "}\n",
        "\n",
        "# =========================================================\n",
        "# 2) ë…¸ì„  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "# =========================================================\n",
        "def get_routes(area):\n",
        "    res = requests.get(LIST_URL.format(area=area), headers=HEADERS)\n",
        "    soup = BeautifulSoup(res.text, \"lxml\")\n",
        "\n",
        "    routes = []\n",
        "    for li in soup.select(\"div.wrap-noti li\"):\n",
        "        a = li.find(\"a\")\n",
        "        if not a:\n",
        "            continue\n",
        "        routes.append((a.get_text(strip=True), a[\"data-id\"]))\n",
        "    return routes\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3) ì‹œê°„í‘œ íŒŒì‹± (4 ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜)\n",
        "# =========================================================\n",
        "def parse_timetable(soup):\n",
        "    table_time = {\n",
        "        \"T1_weekday\": [],\n",
        "        \"T1_weekend\": [],\n",
        "        \"T2_weekday\": [],\n",
        "        \"T2_weekend\": [],\n",
        "    }\n",
        "\n",
        "    for row in soup.select(\".func-table2 tbody tr\"):\n",
        "        th = row.select_one(\"th\")\n",
        "        td = row.select_one(\"td\")\n",
        "        if not th or not td:\n",
        "            continue\n",
        "\n",
        "        title = th.get_text(strip=True)\n",
        "\n",
        "        times = [em.get_text(strip=True) for em in td.select(\"em\")]\n",
        "\n",
        "        if \"T1\" in title and \"í‰ì¼\" in title:\n",
        "            table_time[\"T1_weekday\"] = times\n",
        "        elif \"T1\" in title and \"ì£¼ë§\" in title:\n",
        "            table_time[\"T1_weekend\"] = times\n",
        "        elif \"T2\" in title and \"í‰ì¼\" in title:\n",
        "            table_time[\"T2_weekday\"] = times\n",
        "        elif \"T2\" in title and \"ì£¼ë§\" in title:\n",
        "            table_time[\"T2_weekend\"] = times\n",
        "\n",
        "    return table_time\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4) ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "# =========================================================\n",
        "def get_detail(route_id):\n",
        "    res = requests.post(DETAIL_URL, headers=HEADERS, data={\"routeId\": route_id})\n",
        "    soup = BeautifulSoup(res.text, \"lxml\")\n",
        "\n",
        "    timetable = parse_timetable(soup)\n",
        "\n",
        "    stops = [p.get_text(strip=True) for p in soup.select(\".route-bus-line-list-article p\")]\n",
        "\n",
        "    # T1 / T2 íƒ‘ìŠ¹ìœ„ì¹˜\n",
        "    boarding_T1 = \"\"\n",
        "    boarding_T2 = \"\"\n",
        "\n",
        "    t1 = soup.select(\".route-bus-stop-ico-box p\")\n",
        "    t2 = soup.select(\".route-bus-stop2-ico-box2 p\")\n",
        "\n",
        "    if t1:\n",
        "        boarding_T1 = t1[0].get_text(strip=True)\n",
        "\n",
        "    if t2:\n",
        "        boarding_T2 = t2[0].get_text(strip=True)\n",
        "\n",
        "    return timetable, stops, boarding_T1, boarding_T2\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 5) CSV íŒŒì¼ ì¤€ë¹„\n",
        "# =========================================================\n",
        "csv_file = open(\"airport_bus_routes.csv\", \"w\", encoding=\"utf-8-sig\", newline=\"\")\n",
        "writer = csv.writer(csv_file)\n",
        "\n",
        "writer.writerow([\n",
        "    \"region\", \"bus_no\", \"route_id\",\n",
        "    \"T1_weekday\", \"T1_weekend\",\n",
        "    \"T2_weekday\", \"T2_weekend\",\n",
        "    \"stops\",\n",
        "    \"boarding_T1\", \"boarding_T2\"\n",
        "])\n",
        "\n",
        "# =========================================================\n",
        "# 6) ì „ì²´ ì§€ì—­ í¬ë¡¤ë§\n",
        "# =========================================================\n",
        "for area_code, region_name in REGIONS.items():\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"ğŸ“Œ {region_name} íƒ­ í¬ë¡¤ë§ ì‹œì‘â€¦\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    routes = get_routes(area_code)\n",
        "    print(f\" - ë…¸ì„  ìˆ˜: {len(routes)}ê°œ\\n\")\n",
        "\n",
        "    for bus_no, route_id in routes:\n",
        "        print(f\" â†’ {bus_no} ({route_id}) ìƒì„¸ ì¡°íšŒ ì¤‘â€¦\")\n",
        "\n",
        "        timetable, stops, boarding_T1, boarding_T2 = get_detail(route_id)\n",
        "\n",
        "        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥ë˜ê²Œ ë³€ê²½\n",
        "        writer.writerow([\n",
        "            region_name,\n",
        "            bus_no,\n",
        "            route_id,\n",
        "            json.dumps(timetable[\"T1_weekday\"], ensure_ascii=False),\n",
        "            json.dumps(timetable[\"T1_weekend\"], ensure_ascii=False),\n",
        "            json.dumps(timetable[\"T2_weekday\"], ensure_ascii=False),\n",
        "            json.dumps(timetable[\"T2_weekend\"], ensure_ascii=False),\n",
        "            json.dumps(stops, ensure_ascii=False),\n",
        "            boarding_T1,\n",
        "            boarding_T2\n",
        "        ])\n",
        "\n",
        "        time.sleep(0.25)\n",
        "\n",
        "csv_file.close()\n",
        "\n",
        "print(\"\\nğŸ‰ CSV ì €ì¥ ì™„ë£Œ: airport_bus_routes.csv\")\n"
      ],
      "metadata": {
        "id": "VOHQzXGkzP0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 1. ê²½ë¡œ ì„¤ì • =====\n",
        "BUS_CSV = \"airport_bus_routes.csv\"\n",
        "CHROMA_PATH = \"./chroma_bus_db\" # Colab ë¡œì»¬ ê²½ë¡œì— ì €ì¥\n",
        "COLLECTION_NAME = \"bus_routes\"\n",
        "MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
        "\n",
        "# ===== 2. ì„ë² ë”© í•¨ìˆ˜ í´ë˜ìŠ¤ ì •ì˜ (ChromaDB ê·œê²© ì¤€ìˆ˜) =====\n",
        "class MultilingualE5EmbeddingFunction(EmbeddingFunction):\n",
        "    def __init__(self, model_name=MODEL_NAME):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        # E5 ëª¨ë¸ì€ ë¬¸ì„œ ì €ì¥ ì‹œ 'passage: ' ì ‘ë‘ì–´ê°€ í•„ìš”í•¨\n",
        "        processed_input = [f\"passage: {text}\" for text in input]\n",
        "        embeddings = self.model.encode(processed_input, normalize_embeddings=True)\n",
        "        return embeddings.tolist()\n",
        "\n",
        "# ===== 3. CSV ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ =====\n",
        "def load_and_preprocess_data(filepath):\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}\")\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # ë¬¸ìì—´ë¡œ ëœ ë¦¬ìŠ¤íŠ¸(\"['a', 'b']\")ë¥¼ ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜\n",
        "    def safe_parse(val):\n",
        "        try:\n",
        "            return ast.literal_eval(val) if isinstance(val, str) else []\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    # ë°ì´í„° í”„ë ˆì„ì— ì ìš© (ë¦¬ìŠ¤íŠ¸ íŒŒì‹±)\n",
        "    list_cols = ['T1_weekday', 'T1_weekend', 'T2_weekday', 'T2_weekend', 'stops']\n",
        "    for col in list_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(safe_parse)\n",
        "\n",
        "    print(f\"âœ… CSV ë¡œë“œ ë° íŒŒì‹± ì™„ë£Œ: {len(df)}ê°œ ë…¸ì„ \")\n",
        "    return df\n",
        "\n",
        "# ===== 4. ë¬¸ì¥ ìƒì„± (ì²­í‚¹) =====\n",
        "def create_passage_text(row):\n",
        "    # ë°ì´í„° ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n",
        "    region = row.get(\"region\", \"ì•Œ ìˆ˜ ì—†ìŒ\")\n",
        "    bus_no = row.get(\"bus_no\", \"ë²ˆí˜¸ ì—†ìŒ\")\n",
        "    route_id = row.get(\"route_id\", \"\")\n",
        "    stops = row.get(\"stops\", [])\n",
        "\n",
        "    # ì •ë¥˜ì¥ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë„ˆë¬´ ê¸¸ë©´ ì£¼ìš” ê²½ìœ ì§€ë§Œ í‘œì‹œ ê°€ëŠ¥)\n",
        "    stops_str = \", \".join(stops) if stops else \"ì •ë³´ ì—†ìŒ\"\n",
        "\n",
        "    # ì‹œê°„í‘œ ìš”ì•½ (ì• 5ê°œë§Œ ì˜ˆì‹œë¡œ í¬í•¨í•˜ì—¬ í† í° ì ˆì•½)\n",
        "    t1_times = \", \".join(row.get(\"T1_weekday\", [])[:5])\n",
        "\n",
        "    # ê²€ìƒ‰ì— ìœ ë¦¬í•˜ë„ë¡ ìì—°ì–´ ë¬¸ì¥ êµ¬ì„±\n",
        "    text = (\n",
        "        f\"{region} ì§€ì—­ ê³µí•­ë²„ìŠ¤ {bus_no}ë²ˆ ë…¸ì„  ì •ë³´ì…ë‹ˆë‹¤. \"\n",
        "        f\"ì£¼ìš” ê²½ìœ  ì •ë¥˜ì¥ì€ {stops_str} ì…ë‹ˆë‹¤. \"\n",
        "        f\"ì¸ì²œê³µí•­ ì œ1í„°ë¯¸ë„ í‰ì¼ ì²«ì°¨ ì‹œê°„ëŒ€ëŠ” {t1_times} ë“± ì…ë‹ˆë‹¤. \"\n",
        "        f\"ì œ1í„°ë¯¸ë„ íƒ‘ìŠ¹ ìœ„ì¹˜ëŠ” {row.get('boarding_T1', 'ì •ë³´ ì—†ìŒ')}, \"\n",
        "        f\"ì œ2í„°ë¯¸ë„ íƒ‘ìŠ¹ ìœ„ì¹˜ëŠ” {row.get('boarding_T2', 'ì •ë³´ ì—†ìŒ')} ì…ë‹ˆë‹¤.\"\n",
        "    )\n",
        "    return text\n",
        "\n",
        "# ===== 5. ë©”ì¸ ì‹¤í–‰ =====\n",
        "def run_embedding_pipeline():\n",
        "    # A. ë°ì´í„° ë¡œë“œ\n",
        "    df = load_and_preprocess_data(BUS_CSV)\n",
        "\n",
        "    # B. ChromaDB í´ë¼ì´ì–¸íŠ¸ ë° ì»¬ë ‰ì…˜ ì¤€ë¹„\n",
        "    # PersistentClientë¥¼ ì‚¬ìš©í•˜ì—¬ ë””ìŠ¤í¬ì— ì €ì¥ (Colab ì„¸ì…˜ ì¬ì‹œì‘ ì „ê¹Œì§€ ìœ ì§€)\n",
        "    client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
        "\n",
        "    # ì»¤ìŠ¤í…€ ì„ë² ë”© í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "    embedding_func = MultilingualE5EmbeddingFunction()\n",
        "\n",
        "    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆë‹¤ë©´ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
        "    try:\n",
        "        client.delete_collection(COLLECTION_NAME)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    collection = client.create_collection(\n",
        "        name=COLLECTION_NAME,\n",
        "        embedding_function=embedding_func\n",
        "    )\n",
        "\n",
        "    print(\"=== ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ ===\")\n",
        "\n",
        "    # C. ë°ì´í„° ì¤€ë¹„ (Documents, Metadatas, Ids)\n",
        "    documents = []\n",
        "    metadatas = []\n",
        "    ids = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # 1. ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ìƒì„±\n",
        "        passage = create_passage_text(row)\n",
        "        documents.append(passage)\n",
        "\n",
        "        # 2. ë©”íƒ€ë°ì´í„° ìƒì„± (ì£¼ì˜: ChromaDB ë©”íƒ€ë°ì´í„° ê°’ì€ str, int, float, boolë§Œ ê°€ëŠ¥)\n",
        "        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ë°ì´í„°ëŠ” ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ì €ì¥í•˜ê±°ë‚˜ ì œì™¸í•´ì•¼ ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤.\n",
        "        meta = {\n",
        "            \"bus_no\": str(row['bus_no']),\n",
        "            \"region\": str(row['region']),\n",
        "            \"route_id\": str(row['route_id']),\n",
        "            \"boarding_T1\": str(row.get('boarding_T1', \"\")),\n",
        "            \"boarding_T2\": str(row.get('boarding_T2', \"\")),\n",
        "\n",
        "            # ì •ë¥˜ì¥\n",
        "            \"stops\": \"|\".join(row.get(\"stops\", [])),\n",
        "            \"stops_preview\": str(row.get(\"stops\", []))[:100],\n",
        "\n",
        "            # ğŸ”¥ ì‹œê°„í‘œ (í•µì‹¬ ìˆ˜ì •)\n",
        "            \"T1_weekday\": \"|\".join(row.get(\"T1_weekday\", [])),\n",
        "            \"T1_weekend\": \"|\".join(row.get(\"T1_weekend\", [])),\n",
        "            \"T2_weekday\": \"|\".join(row.get(\"T2_weekday\", [])),\n",
        "            \"T2_weekend\": \"|\".join(row.get(\"T2_weekend\", [])),\n",
        "        }\n",
        "        metadatas.append(meta)\n",
        "\n",
        "        # 3. ID ìƒì„±\n",
        "        ids.append(f\"{row['region']}_{row['bus_no']}_{idx}\")\n",
        "\n",
        "    # D. ë°ì´í„° ì €ì¥ (Upsert)\n",
        "    print(f\"=== {len(documents)}ê°œ ë°ì´í„° ì„ë² ë”© ì‹œì‘ (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”) ===\")\n",
        "\n",
        "    # ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ (ë°ì´í„°ê°€ ë§ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„)\n",
        "    BATCH_SIZE = 100\n",
        "    for i in range(0, len(documents), BATCH_SIZE):\n",
        "        end_idx = min(i + BATCH_SIZE, len(documents))\n",
        "        collection.upsert(\n",
        "            ids=ids[i:end_idx],\n",
        "            documents=documents[i:end_idx],\n",
        "            metadatas=metadatas[i:end_idx]\n",
        "        )\n",
        "        print(f\" -> {end_idx} / {len(documents)} ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "    print(\"\\nğŸ‰ ëª¨ë“  ë°ì´í„° ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ!\")\n",
        "    return collection, embedding_func\n",
        "\n",
        "# ===== 6. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ =====\n",
        "def search_bus(collection, embedding_func, query_text):\n",
        "    print(f\"\\nğŸ” ê²€ìƒ‰ì–´: '{query_text}'\")\n",
        "\n",
        "    # E5 ëª¨ë¸ì€ ì¿¼ë¦¬ ì‹œ 'query: ' ì ‘ë‘ì–´ ê¶Œì¥\n",
        "    query_processed = f\"query: {query_text}\"\n",
        "\n",
        "    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
        "    query_embedding = embedding_func([query_processed])[0] # ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œ\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=3\n",
        "    )\n",
        "\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        print(f\"[{i+1}] ë²„ìŠ¤: {results['metadatas'][0][i]['bus_no']} ({results['metadatas'][0][i]['region']})\")\n",
        "        print(f\"    ë‚´ìš©: {results['documents'][0][i][:100]}...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# ===== ì‹¤í–‰ =====\n",
        "if __name__ == \"__main__\":\n",
        "    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
        "    collection, embed_func = run_embedding_pipeline()\n"
      ],
      "metadata": {
        "id": "Wu9j_QMd5enb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spatial Index"
      ],
      "metadata": {
        "id": "aQBZlngzWF8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install \"h3==3.7.7\"\n",
        "!python \"/content/drive/MyDrive/Colab Notebooks/build_spatial_index_2.py\"\n",
        "\n",
        "# ìƒì„± í™•ì¸\n",
        "p = pathlib.Path(\"/content/spatial_index.json\")\n",
        "print(\"spatial_index.json exists?\", p.exists())\n",
        "if p.exists():\n",
        "    j = json.loads(p.read_text())\n",
        "    print(\"top-level keys:\", list(j.keys())[:5])\n",
        "    print(\"mode:\", j.get(\"mode\"), \"| #keys:\", len(j.get(\"keys\", [])))\n",
        "    if j.get(\"keys\"):\n",
        "        print(\"sample key entry:\", j[\"keys\"][0])"
      ],
      "metadata": {
        "id": "WA-jlfWWWFlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ëª¨ ì‹¤í–‰\n",
        "!python \"/content/drive/MyDrive/Colab Notebooks/nearby_search_spatial.py\"\n",
        "# ì˜ˆì‹œ: íŠ¹ì • íŒŒë¼ë¯¸í„°\n",
        "# !python /content/nearby_query.py \"íƒ‘ìŠ¹êµ¬ 225\" \"ì¹´í˜/ìŒë£Œ\" 2 5 true"
      ],
      "metadata": {
        "id": "Cpf2qMMZZhOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st2OZwERavC7"
      },
      "source": [
        "# **ë¼ìš°í„° í•¨ìˆ˜(ê²€ìƒ‰) í˜¸ì¶œ + ê°„ë‹¨ ë°ëª¨**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. Colab í™˜ê²½ ì„¸íŒ… ===\n",
        "BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/frameworkv3\"\n",
        "if BASE_DIR not in sys.path:\n",
        "    sys.path.insert(0, BASE_DIR)\n",
        "\n",
        "# OpenAI API Key ì„¤ì •\n",
        "%env OPENAI_API_KEY=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\n",
        "assert os.environ.get(\"OPENAI_API_KEY\"), \"í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "# === 2. ëª¨ë“ˆ ì„í¬íŠ¸ ===\n",
        "import flights_search_chroma as fr\n",
        "import facilities_search_chroma as fs\n",
        "import bus_search_chroma as bs\n",
        "\n",
        "# ë³€ê²½ì‚¬í•­ ë°˜ì˜ (íŒŒì¼ ìˆ˜ì • ì‹œ ìë™ ë¦¬ë¡œë“œ)\n",
        "fr = importlib.reload(fr)\n",
        "fs = importlib.reload(fs)\n",
        "bs = importlib.reload(bs)\n",
        "\n",
        "print(\"âœ… flights_search_chroma / facilities_search_chroma import OK\")\n",
        "\n",
        "# === 3. main_router_chroma ë¡œë“œ ===\n",
        "MAIN_ROUTER_PATH = f\"{BASE_DIR}/main_router_chroma.py\"\n",
        "\n",
        "if not os.path.isfile(MAIN_ROUTER_PATH):\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        if not os.path.ismount(\"/content/drive\"):\n",
        "            drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "assert os.path.isfile(MAIN_ROUTER_PATH), f\"main_router_chroma.py not found: {MAIN_ROUTER_PATH}\"\n",
        "spec = importlib.util.spec_from_file_location(\"main_router_chroma\", MAIN_ROUTER_PATH)\n",
        "mr = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(mr)\n",
        "print(f\"[info] loaded main_router_chroma from: {MAIN_ROUTER_PATH}\")\n"
      ],
      "metadata": {
        "id": "fHwUcQTnXlnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 4. í…ŒìŠ¤íŠ¸ ì§ˆì˜ ===\n",
        "tests = [\n",
        "    #\"ë‚´ì¼ ì¶œë°œí•˜ëŠ” JQ048í¸ì˜ í˜„ì¬ ìƒíƒœì™€ í„°ë¯¸ë„ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜.\",    # flight\n",
        "    #\"ëª¨ë ˆ ì‹±ê°€í¬ë¥´ì—ì„œ ëŒì•„ì˜¤ëŠ” ë¹„í–‰ê¸° ì°¾ì•„ì¤˜.\",                  # flight (arrivals)\n",
        "    #\"JQ048í¸ì˜ íƒ‘ìŠ¹ ê²Œì´íŠ¸ ì°¾ì•„ì¤˜\",\n",
        "    #\"ë‚´ì¼ ì•„ì¹¨ì— ì‹±ê°€í¬ë¥´ì—ì„œ ëŒì•„ì˜¤ëŠ” ë¹„í–‰ê¸°\",\n",
        "    #\"ë‚´ì¼ ë°¤ì— ì‹±ê°€í¬ë¥´ì—ì„œ ëŒì•„ì˜¤ëŠ” ë¹„í–‰ê¸°\",\n",
        "    #\"ì˜¤ëŠ˜ ë„ì¿„ ê°€ëŠ” ë¹„í–‰ê¸°ëŠ” ëª‡ê°œì•¼?\",\n",
        "    #\"ëª¨ë ˆ í‹°ì›¨ì´í•­ê³µ ë¹„í–‰ê¸° ëª‡ ê°œì•¼\",                            # flight (departures)\n",
        "    #\"ë„ì°©ê²Œì´íŠ¸ 238 ê·¼ì²˜ í¡ì—°ì‹¤\",                              # facility + nearby\n",
        "    #\"T1 3ì¸µ í™”ì¥ì‹¤\",                            # facility\n",
        "    #\"ì•„ì‹œì•„ë‚˜ ì²´í¬ì¸ ì¹´ìš´í„° ìœ„ì¹˜ ì•Œë ¤ì¤˜.\"                      # facility\n",
        "    #\"ë‚´ì¼ ì¸ì²œìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ëŒ€í•œí•­ê³µ ë¹„í–‰ê¸°\",\n",
        "    \"ê²Œì´íŠ¸ 250 ê·¼ì²˜ ì¹´í˜ ì°¾ì•„ì¤˜\",\n",
        "    #\"ê²Œì´íŠ¸ 112 ìœ„ì¹˜\",\n",
        "    #\"112ë²ˆ ê²Œì´íŠ¸ ìœ„ì¹˜\",\n",
        "    #\"112ë²ˆ íƒ‘ìŠ¹êµ¬ ìœ„ì¹˜\"\n",
        "    #\"ì•„ì‹œì•„ë‚˜ ì²´í¬ì¸ ì¹´ìš´í„°\",\n",
        "    #\"ì•„ì‹œì•„ë‚˜ ë¼ìš´ì§€ ìœ„ì¹˜\".\n",
        "    #\"3ì¸µ ìŠ¤íƒ€ë²…ìŠ¤ ì£¼ë³€ í™”ì¥ì‹¤\",\n",
        "    #\"ë‚´ì¼ ì•„ì¹¨ ì•¼íƒ‘ ê°€ëŠ” ë²„ìŠ¤ ì°¾ì•„ì¤˜\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== ğŸš€ Chroma RAG ì§ˆì˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===\")\n",
        "for t in tests:\n",
        "    START = time.perf_counter()\n",
        "    out = mr.route_and_answer(t, k_fac=20, verbose=True)\n",
        "    print(f\"\\nQ: {t}\")\n",
        "    print(f\"MODE: {out['mode']}\")\n",
        "    print(f\"A: {out['text']}\")\n",
        "    print(f\"[time] {(time.perf_counter()-START)*1000:.1f} ms\")"
      ],
      "metadata": {
        "id": "QMqKnoraaVWA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì§ˆì˜ ì…ë ¥ì°½ ì‹¤í–‰**"
      ],
      "metadata": {
        "id": "Ja4-lzLajqAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== ì…ë ¥ì°½ =====\n",
        "query_input = widgets.Text(\n",
        "    placeholder=\"ì˜ˆ: 25ë²ˆ ê²Œì´íŠ¸ ì•Œë ¤ì¤˜\",\n",
        "    description=\"ì§ˆì˜:\",\n",
        "    layout=widgets.Layout(width=\"80%\"),\n",
        ")\n",
        "\n",
        "# ===== ì‹¤í–‰ ë²„íŠ¼ =====\n",
        "run_button = widgets.Button(\n",
        "    description=\"ì§ˆë¬¸í•˜ê¸°\",\n",
        "    button_style=\"primary\",\n",
        "    icon=\"comment\"\n",
        ")\n",
        "\n",
        "# ===== ì¶œë ¥ ì˜ì—­ (ë‹µë³€ë§Œ) =====\n",
        "output_box = widgets.Output(\n",
        "    layout=widgets.Layout(\n",
        "        border=\"1px solid #ddd\",\n",
        "        padding=\"14px\",\n",
        "        width=\"80%\",\n",
        "        background_color=\"#fafafa\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# ===== ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ =====\n",
        "def on_run_clicked(b):\n",
        "    with output_box:\n",
        "        clear_output()\n",
        "        q = query_input.value.strip()\n",
        "        if not q:\n",
        "            print(\"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            out = mr.route_and_answer(q, k_fac=20, verbose=False)\n",
        "\n",
        "            # âœ… ë‹µë³€ë§Œ ì¶œë ¥\n",
        "            print(out[\"text\"])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n",
        "            print(e)\n",
        "\n",
        "# ì´ë²¤íŠ¸ ì—°ê²°\n",
        "run_button.on_click(on_run_clicked)\n",
        "\n",
        "# ===== UI êµ¬ì„± =====\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h3>âœˆï¸ Incheon Airport Assistant</h3>\"),\n",
        "    query_input,\n",
        "    run_button,\n",
        "    output_box\n",
        "])\n",
        "\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "iPwaUEmECIf-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}